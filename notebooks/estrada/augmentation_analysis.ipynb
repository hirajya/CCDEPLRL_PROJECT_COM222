{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71fea149",
   "metadata": {},
   "source": [
    "# Data Augmentation Analysis for Bruise Detection\n",
    "\n",
    "This notebook compares different data augmentation strategies:\n",
    "1. Baseline (no augmentation)\n",
    "2. Horizontal flip only\n",
    "3. Rotation only (±20 degrees)\n",
    "4. Zoom only (±20%)\n",
    "5. All augmentations combined\n",
    "\n",
    "We'll use the same model architecture and preprocessing from the main CNN notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556db810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b223e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load dataset...\n",
      "Current working directory: C:\\Users\\Rodney Lei Estrada\\xcode\\bruise_detection_project\n",
      "Dataset shape: (442, 224, 224, 3)\n",
      "Number of bruise images: 242\n",
      "Number of normal images: 200\n"
     ]
    }
   ],
   "source": [
    "def load_binary_data(data_dir='dataset/Wound_dataset copy', img_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise ValueError(f\"Dataset directory not found: {data_dir}\")\n",
    "    \n",
    "    # Process bruise images (positive class)\n",
    "    bruise_path = os.path.join(data_dir, 'Bruises')\n",
    "    if not os.path.exists(bruise_path):\n",
    "        raise ValueError(f\"Bruises directory not found: {bruise_path}\")\n",
    "    \n",
    "    for img_name in os.listdir(bruise_path):\n",
    "        if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(bruise_path, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img = img.convert('RGB')\n",
    "                img = img.resize(img_size)\n",
    "                img_array = np.array(img) / 255.0\n",
    "                \n",
    "                images.append(img_array)\n",
    "                labels.append(1)  # 1 for bruise\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    # Process normal skin images (negative class)\n",
    "    normal_path = os.path.join(data_dir, 'Normal')\n",
    "    if not os.path.exists(normal_path):\n",
    "        raise ValueError(f\"Normal directory not found: {normal_path}\")\n",
    "    \n",
    "    for img_name in os.listdir(normal_path):\n",
    "        if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(normal_path, img_name)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img = img.convert('RGB')\n",
    "                img = img.resize(img_size)\n",
    "                img_array = np.array(img) / 255.0\n",
    "                \n",
    "                images.append(img_array)\n",
    "                labels.append(0)  # 0 for normal\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    if not images:\n",
    "        raise ValueError(\"No images were loaded. Check the dataset structure and file formats.\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Try loading the dataset with explicit error handling\n",
    "try:\n",
    "    print(\"Attempting to load dataset...\")\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "    X, y = load_binary_data()\n",
    "    print(\"Dataset shape:\", X.shape)\n",
    "    print(\"Number of bruise images:\", np.sum(y == 1))\n",
    "    print(\"Number of normal images:\", np.sum(y == 0))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please verify the dataset path and structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37cdf1a",
   "metadata": {},
   "source": [
    "## Model Architecture and Training Functions\n",
    "\n",
    "We'll use the same CNN architecture as the main notebook, but train it with different augmentation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c762bc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     53\u001b[39m augmentation_configs = {\n\u001b[32m     54\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbaseline\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     55\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mflip\u001b[39m\u001b[33m'\u001b[39m: {\u001b[33m'\u001b[39m\u001b[33mhorizontal_flip\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m     }\n\u001b[32m     63\u001b[39m }\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Split data only if X and y are not empty\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mX\u001b[49m.size == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y.size == \u001b[32m0\u001b[39m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWarning: X and/or y are empty. Please check your dataset loading step.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m     X_train = X_val = X_test = y_train = y_val = y_test = np.array([])\n",
      "\u001b[31mNameError\u001b[39m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "def create_model(input_shape):\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Simple CNN architecture\n",
    "        layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Flatten layer\n",
    "        layers.Flatten(),\n",
    "        \n",
    "        # Dense layer\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        \n",
    "        # Output layer (binary classification)\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model_with_augmentation(X_train, y_train, X_val, y_val, augmentation_config=None, model_name=\"baseline\"):\n",
    "    # Create data generator with augmentation\n",
    "    if augmentation_config is not None:\n",
    "        train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "            **augmentation_config\n",
    "        )\n",
    "    else:\n",
    "        train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "    \n",
    "    # Create and compile model\n",
    "    input_shape = X_train[0].shape\n",
    "    model = create_model(input_shape)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'Precision', 'Recall']\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_datagen.flow(X_train, y_train, batch_size=32),\n",
    "        epochs=10,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Define augmentation configurations\n",
    "augmentation_configs = {\n",
    "    'baseline': None,\n",
    "    'flip': {'horizontal_flip': True},\n",
    "    'rotation': {'rotation_range': 20},\n",
    "    'zoom': {'zoom_range': 0.2},\n",
    "    'all': {\n",
    "        'horizontal_flip': True,\n",
    "        'rotation_range': 20,\n",
    "        'zoom_range': 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "# Split data only if X and y are not empty\n",
    "if X.size == 0 or y.size == 0:\n",
    "    print(\"Warning: X and/or y are empty. Please check your dataset loading step.\")\n",
    "    X_train = X_val = X_test = y_train = y_val = y_test = np.array([])\n",
    "else:\n",
    "    # First split off test set (10%)\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Split remaining 90% into train (70%) and validation (20%)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.20, random_state=42)\n",
    "\n",
    "    print(\"\\nData split sizes:\")\n",
    "    print(f\"Training set: {len(X_train)} images ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"Validation set: {len(X_val)} images ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"Test set: {len(X_test)} images ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a291ac1c",
   "metadata": {},
   "source": [
    "## Train Models with Different Augmentation Strategies\n",
    "\n",
    "We'll train 5 different models and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e36681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train models with different augmentation strategies\n",
    "for aug_name, aug_config in augmentation_configs.items():\n",
    "    print(f\"\\nTraining model with {aug_name} augmentation...\")\n",
    "    model, history = train_model_with_augmentation(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        augmentation_config=aug_config,\n",
    "        model_name=aug_name\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    # Get predictions for ROC curve\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Store results\n",
    "    results[aug_name] = {\n",
    "        'history': history.history,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_precision': test_precision,\n",
    "        'test_recall': test_recall,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'roc_auc': roc_auc,\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTest Results for {aug_name}:\")\n",
    "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    print(f\"Recall: {test_recall:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05d74bc",
   "metadata": {},
   "source": [
    "## Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f973a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_histories(results):\n",
    "    metrics = ['accuracy', 'loss', 'precision', 'recall']\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        row = idx // 2\n",
    "        col = idx % 2\n",
    "        \n",
    "        for aug_name in results.keys():\n",
    "            history = results[aug_name]['history']\n",
    "            axes[row, col].plot(history[metric], label=f'{aug_name}')\n",
    "            axes[row, col].plot(history[f'val_{metric}'], label=f'{aug_name} (val)', linestyle='--')\n",
    "        \n",
    "        axes[row, col].set_title(f'Model {metric.capitalize()}')\n",
    "        axes[row, col].set_xlabel('Epoch')\n",
    "        axes[row, col].set_ylabel(metric.capitalize())\n",
    "        axes[row, col].legend()\n",
    "        axes[row, col].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(results):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for aug_name in results.keys():\n",
    "        plt.plot(results[aug_name]['fpr'], \n",
    "                results[aug_name]['tpr'],\n",
    "                label=f'{aug_name} (AUC = {results[aug_name][\"roc_auc\"]:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Different Augmentation Strategies')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_performance_comparison(results):\n",
    "    metrics = ['test_accuracy', 'test_precision', 'test_recall', 'roc_auc']\n",
    "    x = np.arange(len(results))\n",
    "    width = 0.2\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [results[aug_name][metric] for aug_name in results.keys()]\n",
    "        ax.bar(x + i*width, values, width, label=metric.replace('test_', '').capitalize())\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Performance Comparison Across Augmentation Strategies')\n",
    "    ax.set_xticks(x + width * (len(metrics)-1)/2)\n",
    "    ax.set_xticklabels(list(results.keys()))\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "plot_training_histories(results)\n",
    "plot_roc_curves(results)\n",
    "plot_performance_comparison(results)\n",
    "\n",
    "# Print summary table\n",
    "summary_data = {\n",
    "    'Augmentation': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'ROC AUC': []\n",
    "}\n",
    "\n",
    "for aug_name, result in results.items():\n",
    "    summary_data['Augmentation'].append(aug_name)\n",
    "    summary_data['Accuracy'].append(f\"{result['test_accuracy']:.4f}\")\n",
    "    summary_data['Precision'].append(f\"{result['test_precision']:.4f}\")\n",
    "    summary_data['Recall'].append(f\"{result['test_recall']:.4f}\")\n",
    "    summary_data['ROC AUC'].append(f\"{result['roc_auc']:.4f}\")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8600a492",
   "metadata": {},
   "source": [
    "## Analysis and Insights\n",
    "\n",
    "From the results above, we can analyze:\n",
    "\n",
    "1. Overall Performance:\n",
    "   - Compare accuracy, precision, and recall across different augmentation strategies\n",
    "   - Look at ROC AUC scores to assess overall discriminative ability\n",
    "\n",
    "2. Training Dynamics:\n",
    "   - How different augmentations affect learning speed\n",
    "   - Whether certain augmentations help prevent overfitting\n",
    "\n",
    "3. Best Augmentation Strategy:\n",
    "   - Which augmentation or combination worked best\n",
    "   - Trade-offs between different metrics\n",
    "\n",
    "4. Recommendations:\n",
    "   - Best augmentation strategy for this specific task\n",
    "   - Potential improvements or modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59945fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Print available paths and verify dataset location\n",
    "import os\n",
    "\n",
    "# Define possible dataset paths\n",
    "current_dir = os.getcwd()\n",
    "absolute_dataset_path = os.path.join('C:\\\\Users\\\\Rodney Lei Estrada\\\\dataset\\\\Wound_dataset')\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"Absolute dataset path: {absolute_dataset_path}\")\n",
    "print(f\"\\nChecking if dataset exists at absolute path: {os.path.exists(absolute_dataset_path)}\")\n",
    "\n",
    "if os.path.exists(absolute_dataset_path):\n",
    "    print(\"\\nContents of dataset directory:\")\n",
    "    print(os.listdir(absolute_dataset_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
