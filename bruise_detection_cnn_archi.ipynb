{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc82cea",
   "metadata": {},
   "source": [
    "# Bruise Detection using differemt CNN Architectures\n",
    "\n",
    "This notebook contains comprehensive comparison framework that tests various CNN designs and analyzes their performance for binary bruise detection (bruise vs normal skin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662cabb",
   "metadata": {},
   "source": [
    "### Import libraries and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1376bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, f1_score\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff002d",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23070a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_binary_data(data_dir='dataset/Wound_dataset copy', img_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process bruise images (positive class)\n",
    "    bruise_path = os.path.join(data_dir, 'Bruises')\n",
    "    if os.path.exists(bruise_path):\n",
    "        for img_name in os.listdir(bruise_path):\n",
    "            if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(bruise_path, img_name)\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    img = img.convert('RGB')\n",
    "                    img = img.resize(img_size)\n",
    "                    img_array = np.array(img) / 255.0\n",
    "                    \n",
    "                    images.append(img_array)\n",
    "                    labels.append(1)  # 1 for bruise\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    # Process normal skin images (negative class)\n",
    "    normal_path = os.path.join(data_dir, 'Normal')\n",
    "    if os.path.exists(normal_path):\n",
    "        for img_name in os.listdir(normal_path):\n",
    "            if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img_path = os.path.join(normal_path, img_name)\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    img = img.convert('RGB')\n",
    "                    img = img.resize(img_size)\n",
    "                    img_array = np.array(img) / 255.0\n",
    "                    \n",
    "                    images.append(img_array)\n",
    "                    labels.append(0)  # 0 for normal\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06b029b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (442, 224, 224, 3)\n",
      "Number of bruise images: 242\n",
      "Number of normal images: 200\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "X, y = load_binary_data()\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"Number of bruise images:\", np.sum(y == 1))\n",
    "print(\"Number of normal images:\", np.sum(y == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a2f31",
   "metadata": {},
   "source": [
    "### Shallow CNN architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a55a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shallow_cnn(input_shape, kernel_size=3, use_batch_norm=False, activation='relu'):\n",
    "    \"\"\"Shallow CNN with 2 conv layers\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # First conv block\n",
    "        layers.Conv2D(32, (kernel_size, kernel_size), activation=activation, padding='same'),\n",
    "    ])\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Second conv block\n",
    "    model.add(layers.Conv2D(64, (kernel_size, kernel_size), activation=activation, padding='same'))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation=activation))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_shallow_cnn_leaky(input_shape, kernel_size=3, use_batch_norm=False):\n",
    "    \"\"\"Shallow CNN with LeakyReLU\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # First conv block\n",
    "        layers.Conv2D(32, (kernel_size, kernel_size), padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.01),\n",
    "    ])\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Second conv block\n",
    "    model.add(layers.Conv2D(64, (kernel_size, kernel_size), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64))\n",
    "    model.add(layers.LeakyReLU(alpha=0.01))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684392a0",
   "metadata": {},
   "source": [
    "### Deep CNN architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28b9ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_cnn(input_shape, kernel_size=3, use_batch_norm=False, activation='relu'):\n",
    "    \"\"\"Deep CNN with 4-5 conv layers\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # First conv block\n",
    "        layers.Conv2D(32, (kernel_size, kernel_size), activation=activation, padding='same'),\n",
    "    ])\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Second conv block\n",
    "    model.add(layers.Conv2D(64, (kernel_size, kernel_size), activation=activation, padding='same'))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Third conv block\n",
    "    model.add(layers.Conv2D(128, (kernel_size, kernel_size), activation=activation, padding='same'))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Fourth conv block\n",
    "    model.add(layers.Conv2D(256, (kernel_size, kernel_size), activation=activation, padding='same'))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Fifth conv block\n",
    "    model.add(layers.Conv2D(512, (kernel_size, kernel_size), activation=activation, padding='same'))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(layers.Dense(128, activation=activation))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(64, activation=activation))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_deep_cnn_leaky(input_shape, kernel_size=3, use_batch_norm=False):\n",
    "    \"\"\"Deep CNN with LeakyReLU\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # First conv block\n",
    "        layers.Conv2D(32, (kernel_size, kernel_size), padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.01),\n",
    "    ])\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Second conv block\n",
    "    model.add(layers.Conv2D(64, (kernel_size, kernel_size), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Third conv block\n",
    "    model.add(layers.Conv2D(128, (kernel_size, kernel_size), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Fourth conv block\n",
    "    model.add(layers.Conv2D(256, (kernel_size, kernel_size), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Fifth conv block\n",
    "    model.add(layers.Conv2D(512, (kernel_size, kernel_size), padding='same'))\n",
    "    model.add(layers.LeakyReLU(alpha=0.01))\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU(alpha=0.01))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(64))\n",
    "    model.add(layers.LeakyReLU(alpha=0.01))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72decd4",
   "metadata": {},
   "source": [
    "### Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca509346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, model_name, X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                            epochs=15, batch_size=32):\n",
    "    \"\"\"Train model and collect metrics\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'Precision', 'Recall']\n",
    "    )\n",
    "    \n",
    "    # Display model summary\n",
    "    print(f\"Parameters: {model.count_params():,}\")\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Record training time\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy, test_precision, test_recall = model.evaluate(\n",
    "        X_test, y_test, verbose=0\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_prob = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate F1 score and AUC\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'Model': model_name,\n",
    "        'Parameters': model.count_params(),\n",
    "        'Training_Time': training_time,\n",
    "        'Test_Accuracy': test_accuracy,\n",
    "        'Test_Precision': test_precision,\n",
    "        'Test_Recall': test_recall,\n",
    "        'F1_Score': f1,\n",
    "        'AUC': roc_auc,\n",
    "        'History': history,\n",
    "        'Predictions': y_pred_prob\n",
    "    }\n",
    "    \n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1127a41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=42)\n",
    "\n",
    "input_shape = X_train[0].shape\n",
    "print(f\"Input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bb4ef",
   "metadata": {},
   "source": [
    "### Train Shallow CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "33c99670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_shallow_cnns(X_train, X_val, X_test, y_train, y_val, y_test, input_shape, results):\n",
    "    \"\"\"Train all shallow CNN configurations\"\"\"\n",
    "    print(\"TRAINING SHALLOW CNN ARCHITECTURES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Shallow CNN configurations\n",
    "    shallow_configs = [\n",
    "        {'kernel': 3, 'batch_norm': False, 'activation': 'relu', 'name': 'shallow_k3_no_bn_relu'},\n",
    "        {'kernel': 3, 'batch_norm': True, 'activation': 'relu', 'name': 'shallow_k3_bn_relu'},\n",
    "        {'kernel': 5, 'batch_norm': False, 'activation': 'relu', 'name': 'shallow_k5_no_bn_relu'},\n",
    "        {'kernel': 5, 'batch_norm': True, 'activation': 'relu', 'name': 'shallow_k5_bn_relu'},\n",
    "        {'kernel': 3, 'batch_norm': False, 'activation': 'leaky', 'name': 'shallow_k3_no_bn_leaky'},\n",
    "        {'kernel': 3, 'batch_norm': True, 'activation': 'leaky', 'name': 'shallow_k3_bn_leaky'},\n",
    "    ]\n",
    "    \n",
    "    for config in shallow_configs:\n",
    "        if config['activation'] == 'leaky':\n",
    "            model = create_shallow_cnn_leaky(input_shape, config['kernel'], config['batch_norm'])\n",
    "        else:\n",
    "            model = create_shallow_cnn(input_shape, config['kernel'], config['batch_norm'], config['activation'])\n",
    "        \n",
    "        result = train_and_evaluate_model(model, config['name'], X_train, X_val, X_test, \n",
    "                                        y_train, y_val, y_test)\n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccefce",
   "metadata": {},
   "source": [
    "### Train Deep CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d227e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_cnns(X_train, X_val, X_test, y_train, y_val, y_test, input_shape, results):\n",
    "    \"\"\"Train all deep CNN configurations\"\"\"\n",
    "    print(\"\\nTRAINING DEEP CNN ARCHITECTURES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Deep CNN configurations\n",
    "    deep_configs = [\n",
    "        {'kernel': 3, 'batch_norm': False, 'activation': 'relu', 'name': 'deep_k3_no_bn_relu'},\n",
    "        {'kernel': 3, 'batch_norm': True, 'activation': 'relu', 'name': 'deep_k3_bn_relu'},\n",
    "        {'kernel': 5, 'batch_norm': False, 'activation': 'relu', 'name': 'deep_k5_no_bn_relu'},\n",
    "        {'kernel': 5, 'batch_norm': True, 'activation': 'relu', 'name': 'deep_k5_bn_relu'},\n",
    "        {'kernel': 3, 'batch_norm': False, 'activation': 'leaky', 'name': 'deep_k3_no_bn_leaky'},\n",
    "        {'kernel': 3, 'batch_norm': True, 'activation': 'leaky', 'name': 'deep_k3_bn_leaky'},\n",
    "    ]\n",
    "    \n",
    "    for config in deep_configs:\n",
    "        if config['activation'] == 'leaky':\n",
    "            model = create_deep_cnn_leaky(input_shape, config['kernel'], config['batch_norm'])\n",
    "        else:\n",
    "            model = create_deep_cnn(input_shape, config['kernel'], config['batch_norm'], config['activation'])\n",
    "        \n",
    "        result = train_and_evaluate_model(model, config['name'], X_train, X_val, X_test, \n",
    "                                        y_train, y_val, y_test)\n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e44b3",
   "metadata": {},
   "source": [
    "### Results summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ca02c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_table(results):\n",
    "    \"\"\"Print detailed comparison table\"\"\"\n",
    "    results_df = pd.DataFrame([{\n",
    "        'Model': r['Model'],\n",
    "        'Parameters': f\"{r['Parameters']:,}\",\n",
    "        'Accuracy': f\"{r['Test_Accuracy']:.4f}\",\n",
    "        'Precision': f\"{r['Test_Precision']:.4f}\",\n",
    "        'Recall': f\"{r['Test_Recall']:.4f}\",\n",
    "        'F1-Score': f\"{r['F1_Score']:.4f}\",\n",
    "        'AUC': f\"{r['AUC']:.4f}\",\n",
    "        'Train Time (s)': f\"{r['Training_Time']:.1f}\"\n",
    "    } for r in results])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"DETAILED RESULTS COMPARISON\")\n",
    "    print(\"=\"*120)\n",
    "    print(results_df.to_string(index=False))\n",
    "    \n",
    "    # Find best models\n",
    "    best_accuracy = max(results, key=lambda x: x['Test_Accuracy'])\n",
    "    best_f1 = max(results, key=lambda x: x['F1_Score'])\n",
    "    best_auc = max(results, key=lambda x: x['AUC'])\n",
    "    fastest = min(results, key=lambda x: x['Training_Time'])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BEST PERFORMING MODELS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best Accuracy: {best_accuracy['Model']} ({best_accuracy['Test_Accuracy']:.4f})\")\n",
    "    print(f\"Best F1-Score: {best_f1['Model']} ({best_f1['F1_Score']:.4f})\")\n",
    "    print(f\"Best AUC: {best_auc['Model']} ({best_auc['AUC']:.4f})\")\n",
    "    print(f\"Fastest Training: {fastest['Model']} ({fastest['Training_Time']:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbfba04",
   "metadata": {},
   "source": [
    "### Batch normalization impact analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81eeb1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_batch_norm(results):\n",
    "    \"\"\"Analyze batch normalization impact\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df['Depth'] = df['Model'].str.split('_').str[0]\n",
    "    df['Batch_Norm'] = df['Model'].str.contains('_bn_')\n",
    "    df['Activation'] = df['Model'].str.split('_').str[-1]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"BATCH NORMALIZATION IMPACT ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall impact\n",
    "    with_bn = df[df['Batch_Norm'] == True]\n",
    "    without_bn = df[df['Batch_Norm'] == False]\n",
    "    \n",
    "    print(f\"Overall Impact:\")\n",
    "    print(f\"  With BatchNorm - Avg Accuracy: {with_bn['Test_Accuracy'].mean():.4f}\")\n",
    "    print(f\"  Without BatchNorm - Avg Accuracy: {without_bn['Test_Accuracy'].mean():.4f}\")\n",
    "    improvement = ((with_bn['Test_Accuracy'].mean() - without_bn['Test_Accuracy'].mean())/without_bn['Test_Accuracy'].mean())*100\n",
    "    print(f\"  Improvement: {improvement:.2f}%\")\n",
    "    \n",
    "    # Impact by architecture depth\n",
    "    print(f\"\\nImpact by Architecture Depth:\")\n",
    "    for depth in ['shallow', 'deep']:\n",
    "        depth_data = df[df['Depth'] == depth]\n",
    "        with_bn_depth = depth_data[depth_data['Batch_Norm'] == True]['Test_Accuracy'].mean()\n",
    "        without_bn_depth = depth_data[depth_data['Batch_Norm'] == False]['Test_Accuracy'].mean()\n",
    "        improvement = ((with_bn_depth - without_bn_depth)/without_bn_depth)*100\n",
    "        \n",
    "        print(f\"  {depth.capitalize()} CNN:\")\n",
    "        print(f\"    With BN: {with_bn_depth:.4f}\")\n",
    "        print(f\"    Without BN: {without_bn_depth:.4f}\")\n",
    "        print(f\"    Improvement: {improvement:.2f}%\")\n",
    "    \n",
    "    # Training time impact\n",
    "    print(f\"\\nTraining Time Impact:\")\n",
    "    print(f\"  With BatchNorm - Avg Time: {with_bn['Training_Time'].mean():.1f}s\")\n",
    "    print(f\"  Without BatchNorm - Avg Time: {without_bn['Training_Time'].mean():.1f}s\")\n",
    "    time_diff = ((with_bn['Training_Time'].mean() - without_bn['Training_Time'].mean())/without_bn['Training_Time'].mean())*100\n",
    "    print(f\"  Time Increase: {time_diff:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b745dc2",
   "metadata": {},
   "source": [
    "### Activation function and kernel size analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce30c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_activation_and_kernels(results):\n",
    "    \"\"\"Analyze activation functions and kernel sizes\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df['Depth'] = df['Model'].str.split('_').str[0]\n",
    "    df['Kernel_Size'] = df['Model'].str.extract(r'k(\\d+)')[0].astype(int)\n",
    "    df['Activation'] = df['Model'].str.split('_').str[-1]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ACTIVATION FUNCTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall comparison\n",
    "    relu_results = df[df['Activation'] == 'relu']\n",
    "    leaky_relu_results = df[df['Activation'] == 'leaky']\n",
    "    \n",
    "    print(f\"Overall Performance:\")\n",
    "    print(f\"  ReLU - Avg Accuracy: {relu_results['Test_Accuracy'].mean():.4f}\")\n",
    "    print(f\"  LeakyReLU - Avg Accuracy: {leaky_relu_results['Test_Accuracy'].mean():.4f}\")\n",
    "    \n",
    "    print(f\"\\nTraining Speed:\")\n",
    "    print(f\"  ReLU - Avg Time: {relu_results['Training_Time'].mean():.1f}s\")\n",
    "    print(f\"  LeakyReLU - Avg Time: {leaky_relu_results['Training_Time'].mean():.1f}s\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"KERNEL SIZE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Kernel size comparison\n",
    "    k3_results = df[df['Kernel_Size'] == 3]\n",
    "    k5_results = df[df['Kernel_Size'] == 5]\n",
    "    \n",
    "    print(f\"Overall Performance:\")\n",
    "    print(f\"  3x3 Kernels - Avg Accuracy: {k3_results['Test_Accuracy'].mean():.4f}\")\n",
    "    print(f\"  5x5 Kernels - Avg Accuracy: {k5_results['Test_Accuracy'].mean():.4f}\")\n",
    "    \n",
    "    print(f\"\\nTraining Speed:\")\n",
    "    print(f\"  3x3 Kernels - Avg Time: {k3_results['Training_Time'].mean():.1f}s\")\n",
    "    print(f\"  5x5 Kernels - Avg Time: {k5_results['Training_Time'].mean():.1f}s\")\n",
    "    \n",
    "    print(f\"\\nParameter Count:\")\n",
    "    print(f\"  3x3 Kernels - Avg Params: {k3_results['Parameters'].mean():,.0f}\")\n",
    "    print(f\"  5x5 Kernels - Avg Params: {k5_results['Parameters'].mean():,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1516c58c",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6488364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comprehensive_comparison(results):\n",
    "    \"\"\"Create comprehensive comparison plots\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Parse model components\n",
    "    df['Depth'] = df['Model'].str.split('_').str[0]\n",
    "    df['Kernel_Size'] = df['Model'].str.extract(r'k(\\d+)')[0].astype(int)\n",
    "    df['Batch_Norm'] = df['Model'].str.contains('_bn_')\n",
    "    df['Activation'] = df['Model'].str.split('_').str[-1]\n",
    "    \n",
    "    # Create comprehensive comparison plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # 1. Accuracy vs Model Depth\n",
    "    depth_comparison = df.groupby(['Depth', 'Batch_Norm']).agg({\n",
    "        'Test_Accuracy': 'mean',\n",
    "        'Parameters': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    shallow_bn = depth_comparison[(depth_comparison['Depth'] == 'shallow') & (depth_comparison['Batch_Norm'] == True)]['Test_Accuracy'].values[0]\n",
    "    shallow_no_bn = depth_comparison[(depth_comparison['Depth'] == 'shallow') & (depth_comparison['Batch_Norm'] == False)]['Test_Accuracy'].values[0]\n",
    "    deep_bn = depth_comparison[(depth_comparison['Depth'] == 'deep') & (depth_comparison['Batch_Norm'] == True)]['Test_Accuracy'].values[0]\n",
    "    deep_no_bn = depth_comparison[(depth_comparison['Depth'] == 'deep') & (depth_comparison['Batch_Norm'] == False)]['Test_Accuracy'].values[0]\n",
    "    \n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    ax.bar(x - width/2, [shallow_no_bn, deep_no_bn], width, label='Without BatchNorm', alpha=0.7)\n",
    "    ax.bar(x + width/2, [shallow_bn, deep_bn], width, label='With BatchNorm', alpha=0.7)\n",
    "    ax.set_title('Accuracy vs Model Depth')\n",
    "    ax.set_ylabel('Test Accuracy')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['Shallow', 'Deep'])\n",
    "    ax.legend()\n",
    "    \n",
    "    # 2. Parameters vs Depth\n",
    "    ax = axes[0, 1]\n",
    "    shallow_params = df[df['Depth'] == 'shallow']['Parameters'].mean()\n",
    "    deep_params = df[df['Depth'] == 'deep']['Parameters'].mean()\n",
    "    ax.bar(['Shallow CNN', 'Deep CNN'], [shallow_params, deep_params], \n",
    "           color=['lightblue', 'darkblue'])\n",
    "    ax.set_title('Parameter Count Comparison')\n",
    "    ax.set_ylabel('Number of Parameters')\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M' if x >= 1e6 else f'{x/1e3:.0f}K'))\n",
    "    \n",
    "    # 3. Impact of Batch Normalization\n",
    "    ax = axes[0, 2]\n",
    "    bn_data = df.groupby(['Batch_Norm', 'Depth'])['Test_Accuracy'].mean().unstack()\n",
    "    bn_data.plot(kind='bar', ax=ax, color=['lightcoral', 'darkred'])\n",
    "    ax.set_title('Impact of Batch Normalization')\n",
    "    ax.set_xlabel('Batch Normalization')\n",
    "    ax.set_ylabel('Test Accuracy')\n",
    "    ax.set_xticklabels(['Without BN', 'With BN'], rotation=0)\n",
    "    ax.legend(title='Architecture')\n",
    "    \n",
    "    # 4. Kernel Size Comparison\n",
    "    ax = axes[1, 0]\n",
    "    kernel_data = df.groupby(['Kernel_Size', 'Depth'])['Test_Accuracy'].mean().unstack()\n",
    "    kernel_data.plot(kind='bar', ax=ax, color=['lightgreen', 'darkgreen'])\n",
    "    ax.set_title('Kernel Size Impact')\n",
    "    ax.set_xlabel('Kernel Size')\n",
    "    ax.set_ylabel('Test Accuracy')\n",
    "    ax.set_xticklabels(['3x3', '5x5'], rotation=0)\n",
    "    ax.legend(title='Architecture')\n",
    "    \n",
    "    # 5. Activation Function Comparison\n",
    "    ax = axes[1, 1]\n",
    "    activation_data = df.groupby(['Activation', 'Depth'])['Test_Accuracy'].mean().unstack()\n",
    "    activation_data.plot(kind='bar', ax=ax, color=['orange', 'darkorange'])\n",
    "    ax.set_title('Activation Function Impact')\n",
    "    ax.set_xlabel('Activation Function')\n",
    "    ax.set_ylabel('Test Accuracy')\n",
    "    ax.set_xticklabels(['LeakyReLU', 'ReLU'], rotation=0)\n",
    "    ax.legend(title='Architecture')\n",
    "    \n",
    "    # 6. Training Time vs Accuracy\n",
    "    ax = axes[1, 2]\n",
    "    colors = ['red' if d == 'deep' else 'blue' for d in df['Depth']]\n",
    "    scatter = ax.scatter(df['Training_Time'], df['Test_Accuracy'], \n",
    "                       c=colors, alpha=0.7, s=100)\n",
    "    ax.set_xlabel('Training Time (seconds)')\n",
    "    ax.set_ylabel('Test Accuracy')\n",
    "    ax.set_title('Training Time vs Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
